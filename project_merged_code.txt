Directory: _Stage
Subdirectories:
Files:
  - config.json
  - generate_html_patterns.py
  - html_patterns.json
  - Readme.md
  - test.py
  - test_patterns.py
  - versioning.json

--------------------------------------------------

Content of config.json:
{
  "settings": {
    "_comment": "تنظیمات کلی برای پروژه استخراج داده‌ها",
    "scraping_method": "requests+beautifulsoup",
    "specific_requirements": {
      "_comment": "تنظیمات خاص برای روش استخراج داده‌ها",
      "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36"
    },
    "database": {
      "_comment": "تنظیمات پایگاه داده برای ذخیره داده‌های استخراج شده",
      "type": "PostgreSQL",
      "host": "localhost",
      "port": 5432,
      "user": "username",
      "password": "password",
      "dbname": "web_scraping_db"
    }
  },
  "websites": [
    {
      "_comment": "جزئیات سایت مورد نظر برای استخراج داده",
      "site_id": "namnak",
      "url": "https://www.namnak.com/",
      "auto_generate_patterns_url": "https://namnak.com/c1-%D8%A7%D8%AE%D8%A8%D8%A7%D8%B1",
      "scraping_method": "requests+beautifulsoup",
      "interval": "5m",
      "pages": [
        {
          "_comment": "صفحه اخبار در سایت نمناک",
          "title": "اخبار",
          "url": "https://namnak.com/c1-%D8%A7%D8%AE%D8%A8%D8%A7%D8%B1"
        },
        {
          "_comment": "صفحه فرهنگ و هنر در سایت نمناک",
          "title": "فرهنگ-و-هنر",
          "url": "https://namnak.com/c6-%D9%81%D8%B1%D9%87%D9%86%DA%AF-%D9%88-%D9%87%D9%86%D8%B1"
        },
        {
          "_comment": "صفحه سرگرمی در سایت نمناک",
          "title": "سرگرمی",
          "url": "https://namnak.com/c4-%D8%B3%D8%B1%DA%AF%D8%B1%D9%85%DB%8C"
        }
      ],
      "auth": {
        "_comment": "اطلاعات ورود برای سایت نمناک (در صورت نیاز)",
        "username": "your_username",
        "password": "your_password"
      },
      "data_to_scrape": [
        {
          "_comment": "عنوان مطالب برای استخراج",
          "name": "title",
          "source": "list",
          "type": "text"
        },
        {
          "_comment": "خلاصه مطالب برای استخراج",
          "name": "summery",
          "source": "list",
          "type": "text"
        },
        {
          "_comment": "لینک‌های مربوط به مطالب برای استخراج",
          "name": "url",
          "source": "list",
          "type": "url"
        },
        {
          "_comment": "متن کامل مطلب که باید از صفحه مرتبط استخراج شود",
          "name": "content",
          "source": "linked_page",
          "type": "text"
        },
        {
          "_comment": "تصاویر مرتبط با مطلب که باید از صفحه مرتبط استخراج شوند",
          "name": "image",
          "source": "linked_page",
          "type": "image"
        },
        {
          "_comment": "تصاویر کوچک مرتبط با مطلب که باید از صفحه مرتبط استخراج شوند",
          "name": "thumbnail",
          "source": "linked_page",
          "type": "image"
        },
        {
          "_comment": "تاریخ مربوط به مطالب برای استخراج",
          "name": "date",
          "source": "list",
          "type": "text"
        }
      ]
    }
  ]
}


Content of generate_html_patterns.py:
import requests
from lxml import html
import json

def fetch_html_tree(url):
    """
    دریافت محتوای HTML صفحه و تبدیل آن به یک درخت برای پردازش با XPath.
    """
    response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    if response.status_code != 200:
        raise Exception(f"Failed to fetch the page: {url}")
    return html.fromstring(response.content)

def auto_generate_patterns(tree):
    """
    تولید خودکار الگوهای XPath برای فیلدهای متداول.
    """
    patterns = {}

    # جستجوی عنوان
    title_element = tree.xpath("//h1 | //h2 | //title")
    if title_element:
        patterns["title"] = {"xpath": tree.getroottree().getpath(title_element[0]), "attribute": "text"}

    # جستجوی URL
    url_element = tree.xpath("//a[contains(@href, 'http')]")
    if url_element:
        patterns["url"] = {"xpath": tree.getroottree().getpath(url_element[0]), "attribute": "href"}

    # جستجوی خلاصه
    summary_element = tree.xpath("//meta[@name='description']/@content | //p")
    if summary_element:
        patterns["summary"] = {"xpath": tree.getroottree().getpath(summary_element[0]), "attribute": "text"}

    # جستجوی محتوا
    content_element = tree.xpath("//div[contains(@class, 'content') or contains(@id, 'content')]")
    if content_element:
        patterns["content"] = {"xpath": tree.getroottree().getpath(content_element[0]), "attribute": "html"}

    # جستجوی تصویر
    image_element = tree.xpath("//img[contains(@src, 'http')]")
    if image_element:
        patterns["image"] = {"xpath": tree.getroottree().getpath(image_element[0]), "attribute": "src"}

    return patterns

def save_patterns_to_config(site_id, patterns, config_file="config.json"):
    """
    ذخیره الگوهای شناسایی‌شده در فایل config.json.
    """
    try:
        with open(config_file, "r", encoding="utf-8") as file:
            config = json.load(file)
    except FileNotFoundError:
        raise Exception(f"Configuration file {config_file} not found.")

    # به‌روزرسانی الگوها برای سایت مشخص‌شده
    for site in config["websites"]:
        if site["site_id"] == site_id:
            site["patterns"] = patterns
            break

    with open(config_file, "w", encoding="utf-8") as file:
        json.dump(config, file, ensure_ascii=False, indent=2)

def process_sites_from_config(config_file="config.json"):
    """
    پردازش تمامی سایت‌های تعریف‌شده در فایل config.json و تولید الگوها.
    """
    try:
        with open(config_file, "r", encoding="utf-8") as file:
            config = json.load(file)
    except FileNotFoundError:
        raise Exception(f"Configuration file {config_file} not found.")

    for site in config["websites"]:
        site_id = site["site_id"]
        url = site.get("url")
        if not url:
            print(f"Skipping {site_id}: No URL provided.")
            continue

        print(f"Processing {site_id}...")
        try:
            tree = fetch_html_tree(url)
            patterns = auto_generate_patterns(tree)
            save_patterns_to_config(site_id, patterns, config_file)
            print(f"Patterns for {site_id} saved successfully.")
        except Exception as e:
            print(f"Failed to process {site_id}: {e}")

if __name__ == "__main__":
    process_sites_from_config()


Content of html_patterns.json:
{
  "websites": [
    {
      "site_id": "namnak",
      "patterns": {
        "title": {
          "xpath": "//article/div[2]/section[1]/div/a/h2",
          "attribute": "text",
          "source": "list"
        },
        "url": {
          "xpath": "//article/div[2]/section[1]/div/a",
          "attribute": "href",
          "source": "list"
        },
        "summery": {
          "xpath": "//article/div[2]/section[1]/div/div[2]",
          "attribute": "text",
          "source": "list"
        },
        "content": {
          "xpath": "//article[1]",
          "attribute": "html",
          "source": "linked_page"
        },
        "date": {
          "xpath": "//article/div[2]/section[1]/div/div[1]/span[1]",
          "attribute": "text",
          "source": "list"
        },
        "thumbnail": {
          "xpath": "//article/div[2]/section[9]/div/a/img",
          "attribute": "src",
          "source": "list"
        },
        "image": {
          "xpath": "//header/div/div[1]/img",
          "attribute": "src",
          "source": "linked_page"
        }
      }
    }
  ]
}

Content of Readme.md:
# Web Scraping Project

## Overview
This project is a comprehensive web scraping solution designed for collecting, analyzing, and storing data from websites. It is built with modular components to ensure flexibility and scalability.

## Files in the Project
- `config.json`: Stores configuration settings including scraping methods, database credentials, and website-specific details.
- `database.py`: Handles database operations, including saving scraped data into a PostgreSQL database.
- `generate_html_patterns.py`: Generates HTML patterns (e.g., tags, classes) for structured data extraction.
- `html_patterns.json`: Stores the HTML patterns for various websites in JSON format.
- `main.py`: The main script orchestrating the scraping process.
- `scraper.py`: Contains the scraping logic to fetch and parse website data.
- `structure_learning.py`: Implements the logic to learn and adapt to changes in website structures automatically.
- `utils.py`: Provides utility functions, such as saving data to files.
- `validate_html_patterns.py`: Validates the stored HTML patterns against live websites.

## How to Run
1. Install the required Python packages:
    ```bash
    pip install -r requirements.txt
    ```
2. Update the `config.json` file with your specific settings (e.g., database credentials, website URLs).
    - Example:
      ```json
      {
        "settings": {
          "scraping_method": "requests+beautifulsoup",
          "specific_requirements": {
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36"
          },
          "database": {
            "type": "PostgreSQL",
            "host": "localhost",
            "port": 5432,
            "user": "your_user",
            "password": "your_password",
            "dbname": "your_dbname"
          }
        },
        "websites": [
          {
            "url": "https://example.com",
            "scraping_method": "requests+beautifulsoup",
            "interval": "5m",
            "pages": [
              {
                "title": "Page Title",
                "url": "https://example.com/page"
              }
            ]
          }
        ]
      }
      ```
3. Run the main script:
    ```bash
    python main.py
    ```

## Dependencies
- Python 3.8+
- Libraries:
  - `requests`
  - `beautifulsoup4`
  - `psycopg2`

## Notes
- Ensure the database is set up and accessible before running the scripts.
- Use `generate_html_patterns.py` to generate patterns for new websites.
- Validate patterns using `validate_html_patterns.py` to avoid scraping errors.

## License
This project is licensed under the MIT License.


Content of test.py:
import json

# Load the config.json file
with open("config.json", "r", encoding="utf-8") as config_file:
    config = json.load(config_file)

# Load the html_pattern.json file
with open("html_patterns.json", "r", encoding="utf-8") as pattern_file:
    html_pattern = json.load(pattern_file)

# Iterate through websites in config and html_pattern to add data sources
for website in html_pattern["websites"]:
    site_id = website["site_id"]

    # Find corresponding website in config.json
    matching_site = next(
        (site for site in config["websites"] if site["site_id"] == site_id), None
    )

    if matching_site:
        data_sources = {item["name"]: item["source"] for item in matching_site["data_to_scrape"]}

        # Add source to each field in the patterns
        for field, details in website["patterns"].items():
            if field in data_sources:
                details["source"] = data_sources[field]

# Save the updated html_pattern.json
with open("html_pattern_updated.json", "w", encoding="utf-8") as updated_pattern_file:
    json.dump(html_pattern, updated_pattern_file, ensure_ascii=False, indent=2)

print("html_pattern_updated.json has been created with added data sources.")


Content of test_patterns.py:
import json
import requests
from urllib.parse import urljoin
from lxml import html


def load_json(file_path):
    """
    بارگذاری داده‌ها از یک فایل JSON.
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return json.load(file)
    except Exception as e:
        print(f"خطا در بارگذاری فایل {file_path}: {e}")
        return {}


def fetch_html_tree(url):
    """
    دریافت محتوای HTML صفحه و تبدیل آن به یک درخت برای پردازش با XPath.
    """
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        if response.status_code != 200:
            print(f"خطا در دریافت صفحه {url}: {response.status_code}")
            return None
        return html.fromstring(response.content)
    except Exception as e:
        print(f"خطا در دریافت صفحه {url}: {e}")
        return None


def validate_patterns(tree, patterns, source):
    """
    بررسی الگوها بر روی یک درخت HTML خاص.
    """
    extracted_data = {}
    for key, details in patterns.items():
        xpath = details.get("xpath")
        attribute = details.get("attribute")

        if details.get("source") != source:
            continue

        if not xpath or not attribute:
            print(f"الگوی {key} ناقص است.")
            continue

        # استخراج داده‌ها با استفاده از XPath
        elements = tree.xpath(xpath)
        if elements:
            if attribute == "text":
                extracted_data[key] = elements[0].text.strip() if elements[0].text else ""
            elif attribute == "html":
                extracted_data[key] = elements[0].text_content().strip() if elements[0].text_content() else ""
            else:
                extracted_data[key] = elements[0].get(attribute)
        else:
            extracted_data[key] = None

        # print(f"******* {key}  : {extracted_data[key]}")

    return extracted_data


def process_linked_pages(linked_urls, linked_patterns, base_url):
    """
    پردازش صفحات لینک شده و استخراج داده‌ها از آن‌ها.
    """
    linked_data = []
    for url in linked_urls:
        full_url = urljoin(base_url, url)  # ترکیب لینک نسبی با آدرس اصلی
        print(f"  پردازش صفحه لینک‌شده: {full_url}")
        tree = fetch_html_tree(full_url)
        if tree is not None:
            data = validate_patterns(tree, linked_patterns, 'linked_page')
            linked_data.append( data)
        else:
            print(f"    خطا در بارگذاری صفحه لینک‌شده: {full_url}")
    return linked_data


def test_config_and_patterns():
    """
    تست فایل‌های config.json و html_patterns.json بر روی صفحات تعریف‌شده.
    """
    config = load_json('config.json')
    patterns = load_json('html_patterns.json')

    websites = config.get("websites", [])
    html_patterns = {site["site_id"]: site["patterns"] for site in patterns.get("websites", [])}

    for site in websites:
        site_id = site.get("site_id")
        site_url = site.get("url")
        pages = site.get("pages", [])

        if not site_id or site_id not in html_patterns:
            print(f"  الگو برای سایت {site_url} با site_id={site_id} پیدا نشد.")
            continue

        print(f"در حال بررسی سایت: {site_url} (site_id: {site_id})")
        for page in pages:
            page_url = page.get("url")
            print(f"  بررسی صفحه: {page_url}")
            tree = fetch_html_tree(page_url)
            if tree is None:
                print(f"    خطا در بارگذاری صفحه {page_url}")
                continue

            page_patterns = html_patterns[site_id]
            results = validate_patterns(tree, page_patterns, 'list')
            if results:
                print(f"    داده‌های استخراج‌شده از صفحه اصلی:")

                # بررسی لینک‌های استخراج‌شده و پردازش صفحات لینک شده
                if "url" in results and results["url"]:
                    linked_urls = [results["url"]] if isinstance(results["url"], str) else results["url"]
                    linked_data = process_linked_pages(linked_urls, page_patterns, site_url)
                    print(f"    داده‌های استخراج‌شده از صفحات لینک‌شده:")
                    for data in linked_data:
                        results = results | data
                        print(data)

                for key, value in results.items():
                    print(f"      {key}: {value[:200]}")

            else:
                print(f"    خطا در استخراج داده از {page_url}")
        print('-' * 50)


if __name__ == "__main__":
    test_config_and_patterns()


Content of versioning.json:
{
  "project_name": "بازدید پلاس",
  "versioning": [
    {
      "version": "0.1",
      "phase": "R&D",
      "milestone": "طراحی الگوریتم تطبیق HTML",
      "details": [
        "بررسی ساختار سایت‌های هدف و شناسایی الگوها.",
        "طراحی الگوریتمی برای تشخیص تغییرات در ساختار HTML.",
        "ایجاد نمونه اولیه برای تست الگوریتم."
      ],
      "status": "در حال انجام"
    },
    {
      "version": "0.2",
      "phase": "R&D",
      "milestone": "توسعه نمونه اولیه اسکریپت جمع‌آوری داده‌ها",
      "details": [
        "نوشتن اسکریپت جمع‌آوری داده‌ها با استفاده از Requests و BeautifulSoup.",
        "ایجاد قابلیت تطبیق خودکار در صورت تغییر ساختار صفحات.",
        "بررسی صحت عملکرد اسکریپت با داده‌های آزمایشی."
      ],
      "status": "در انتظار"
    },
    {
      "version": "1.0",
      "phase": "MVP",
      "milestone": "ایجاد نسخه اولیه محصول",
      "details": [
        "ایجاد سیستم جمع‌آوری داده‌ها با ذخیره در دیتابیس.",
        "توسعه رابط کاربری برای نمایش داده‌ها.",
        "افزودن قابلیت‌های اولیه تعامل کاربران."
      ],
      "status": "برنامه‌ریزی‌شده"
    },
    {
      "version": "2.0",
      "phase": "توسعه کامل",
      "milestone": "افزودن قابلیت‌های پیشرفته",
      "details": [
        "شخصی‌سازی محتوا با هوش مصنوعی.",
        "اضافه کردن مکانیزم‌های درآمدزایی (بازاریابی وابسته، اشتراک ویژه).",
        "بهینه‌سازی تجربه کاربری و طراحی برای جذب کاربران بیشتر."
      ],
      "status": "برنامه‌ریزی‌شده"
    }
  ]
}


==================================================
